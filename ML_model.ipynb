{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from TorchCRF import CRF\n",
    "\n",
    "from ML_utils import get_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(self,                          # CharCNN 클래스의 인스턴스를 생성했을 때 갖게 되는 성질을 정의하는 메서드\n",
    "                max_word_len = 10,\n",
    "                kernel_lst = \"2,3,4\",\n",
    "                num_filters=32,\n",
    "                char_vocab_size = 1000,\n",
    "                char_emb_dim = 30,\n",
    "                final_char_dim = 50\n",
    "                ):\n",
    "        super(CharCNN, self).__init__()         #nn.Module 내에 있는 메서드를 상속받아 이용한다.\n",
    "\n",
    "        # Initialize character embedding\n",
    "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=0)\n",
    "        nn.init.uniform_(self.char_emb.weight, -0.25, 0.25)\n",
    "\n",
    "        kernel_lst = list(map(int, kernel_lst.split(\",\")))          # \"2, 3, 4\" -> [2, 3, 4]\n",
    "\n",
    "        # Convolution for each kernel\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(char_emb_dim, num_filters, kernel_size, padding=kernel_size // 2),\n",
    "                nn.Tanh(),      # As the paper mentioned\n",
    "                nn.MaxPool1d(max_word_len - kernel_size + 1),\n",
    "                nn.Dropout(0.25),       # As same as the original code implementation\n",
    "            ) for kernel_size in kernel_lst\n",
    "        ])\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(num_filters * len(kernel_lst), 100),\n",
    "            nn.ReLU(),          # As same as the original code implementation\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(100, final_char_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: (batch_size, max_seq_len, max_word_len)\n",
    "        :return: (batch_size, max_seq_len, final_char_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        max_seq_len = x.size(1)\n",
    "        max_word_len = x.size(2)\n",
    "\n",
    "        x = self.char_emb(x)        # (b, s, w, d)\n",
    "        x = x.view(batch_size * max_seq_len, max_word_len, -1)      # (b*s, w, d)\n",
    "        x = x.transpose(2, 1)       # (b*s, d, w): Conv1d takes in (batch, dim, seq_len), but raw embedded is (batch, seq_len, dim)\n",
    "\n",
    "        conv_lst = [conv(x) for conv in self.convs]\n",
    "        conv_concat = torch.cat(conv_lst, dim = -1)     # (b*s, num_filters, len(kernel_lst))\n",
    "        conv_concat = conv_concat.view(conv_concat.size(0), -1)     # (b*s, num_filters * len(kernel_lst))\n",
    "\n",
    "        output = self.linear(conv_concat)       # (b*s, final_char_dim)\n",
    "        output = output.view(batch_size, max_seq_len, -1)   # (b, s, final_char_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CNN_CRF(nn.Module):\n",
    "    def __init__(self, args, pretrained_word_matrix):\n",
    "        super(BiLSTM_CNN_CRF, self).__init__()\n",
    "        self.args = args\n",
    "        \n",
    "        self.char_cnn = CharCNN(max_word_len=args.max_word_len,\n",
    "                                kernel_lst=args.kernel_lst,\n",
    "                                num_filters=args.num_filters,\n",
    "                                char_vocab_size=args.char_vocab_size,\n",
    "                                char_emb_dim=args.char_emb_dim,\n",
    "                                final_char_dim=args.final_char_dim)\n",
    "\n",
    "        if pretrained_word_matrix is not None:\n",
    "            self.word_emb = nn.Embedding.from_pretrained(pretrained_word_matrix)\n",
    "        else:\n",
    "            self.word_emb = nn.Embedding(args.word_vocab_size, args.word_emb_dim, padding_idx = 0)\n",
    "            nn.init.uniform_(self.word_emb.weight, -0.25, 0.25)\n",
    "\n",
    "        self.bi_lstm = nn.LSTM(input_size = args.word_emb_dim + args.final_char_dim,\n",
    "                                hidden_size = args.hidden_dim // 2,         # Bidirectional will double the hidden_size\n",
    "                                bidirectional = True,\n",
    "                                batch_first = True)\n",
    "\n",
    "        self.output_linear = nn.Linear(args.hidden_dim, len(get_labels(args)))\n",
    "\n",
    "        self.crf = CRF(num_tags = len(get_labels(args)), batch_first=True)\n",
    "\n",
    "\n",
    "    def forward(self, word_ids, char_ids, mask, label_ids):\n",
    "        \"\"\"\n",
    "        :param word_ids: (batch_size, max_seq_len)\n",
    "        :param char_ids: (batch_size, max_seq_len, max_word_len)\n",
    "        :param mask: (batch_size, max_seq_len)\n",
    "        :param label_ids: (batch_size, max_seq_len)\n",
    "        :return: (batch_size, max_seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        w_emb = self.word_emb(word_ids)\n",
    "        c_emb = self.char_cnn(char_ids)\n",
    "\n",
    "        w_c_emb = torch.cat([w_emb, c_emb], dim = -1)\n",
    "\n",
    "        lstm_output, _ = self.bi_lstm(w_c_emb, None)\n",
    "\n",
    "        output = self.output_linear(lstm_output)\n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        if label_ids is not None:\n",
    "            loss = self.crf(output, label_ids, mask.byte(), reduction = 'mean')\n",
    "            loss = loss * -1            # negative log likelihood\n",
    "\n",
    "        return loss, output            "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a44d44f71612b50f5c792a4f57cb373375893a6c1b3e54652410687066e7861"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
