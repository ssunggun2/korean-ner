{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            args: NERTrainArguments,\n",
    "            tokenizer: BertTokenizer,\n",
    "            corpus: NERCorpus,\n",
    "            mode: Optional[str] = \"train\",\n",
    "            convert_examples_to_features_fn=_convert_examples_to_ner_features,\n",
    "    ):\n",
    "        if corpus is not None:\n",
    "            self.corpus = corpus\n",
    "        else:\n",
    "            raise KeyError(\"corpus is not valid\")\n",
    "        if not mode in [\"train\", \"val\", \"test\"]:\n",
    "            raise KeyError(f\"mode({mode}) is not a valid split name\")\n",
    "        # Load data features from cache or dataset file\n",
    "        cached_features_file = os.path.join(\n",
    "            args.downstream_corpus_root_dir,\n",
    "            args.downstream_corpus_name,\n",
    "            \"cached_{}_{}_{}_{}_{}\".format(\n",
    "                mode,\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(args.max_seq_length),\n",
    "                args.downstream_corpus_name,\n",
    "                args.downstream_task_name,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Make sure only the first process in distributed training processes the dataset,\n",
    "        # and the others will use the cache.\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "\n",
    "            if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
    "                \n",
    "                start = time.time()\n",
    "                self.features = torch.load(cached_features_file)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                corpus_path = os.path.join(\n",
    "                    args.downstream_corpus_root_dir,\n",
    "                    args.downstream_corpus_name,\n",
    "                )\n",
    "                logger.info(f\"Creating features from dataset file at {corpus_path}\")\n",
    "                examples = self.corpus.get_examples(corpus_path, mode)\n",
    "                self.features = convert_examples_to_features_fn(\n",
    "                    examples,\n",
    "                    tokenizer,\n",
    "                    args,\n",
    "                    label_list=self.corpus.get_labels(),\n",
    "                )\n",
    "                start = time.time()\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file, it could take a lot of time...\"\n",
    "                )\n",
    "                torch.save(self.features, cached_features_file)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.corpus.get_labels()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
