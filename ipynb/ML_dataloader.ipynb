{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd\n",
    "from ML_utils import *\n",
    "from torch.utils.data import TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, words, labels):\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.lables = labels\n",
    "    \n",
    "    def __repr(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serialize this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serialize this instance to a Json string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + '\\n'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, word_ids, char_ids, mask, label_ids):\n",
    "        self.word_ids = word_ids\n",
    "        self.char_ids = char_ids\n",
    "        self.mask = mask\n",
    "        self.label_ids = label_ids\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python diction\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dump(self.to_dict(), intent=2, sort_keys=True) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNerProcessor(object):\n",
    "    \"\"\"Processor for thr Naver NER data set\"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.labels_lst = get_labels(args)\n",
    "\n",
    "    @classmethod\n",
    "    def _read_file(cls, input_file):\n",
    "        \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            for line in f:\n",
    "                lines.append(line.strip())\n",
    "            return lines\n",
    "    \n",
    "    def _create_examples(self, dataset, set_type):\n",
    "        \"\"\"Create examples for thr training and dev sets\"\"\"\n",
    "        examples = []\n",
    "        for (i, data) in enumerate(dataset):\n",
    "            words, labels = data.split('\\t')\n",
    "            words = words.split()\n",
    "            labels = labels.split()\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "\n",
    "            assert len(words) == len(labels)\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                logger.info(data)\n",
    "            examples.append(InputExample(guid = guid, words = words, labels = labels))\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: trina, dev, test\n",
    "        \"\"\"\n",
    "\n",
    "        file_to_read = None\n",
    "        if mode == \"train\":\n",
    "            file_to_read = self.args.train_file\n",
    "        elif mode == \"dev\":\n",
    "            file_to_read = self.args.dev_file\n",
    "        elif mode == \"test\":\n",
    "            file_to_read = self.args.test_file\n",
    "\n",
    "        logger.info(f\"LOOKING AT {(os.path.join(self.args.data_dif, file_to_read))}\")\n",
    "        return self._create_examples(self._read_file(os.path.join(self.args.data_dir, file_to_read)), mode)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_matrix(args, word_vocab):\n",
    "    if not os.path.exists(args.wordvec_dir):\n",
    "        os.mkdir(args.wordvec_dir)\n",
    "\n",
    "    # Making new word vector (as list type)\n",
    "    logger.info(\"Building word matrix...\")\n",
    "    embedding_index = dict()\n",
    "    with open(os.path.join(args.wordvec_dir, args.w2v_file) , 'r', encoding='utf-8', errors = 'ignore') as f:\n",
    "        for line in f:\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            embedding_index[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "    word_matrix = np.zeros((args.word_vocab_size, args.word_emb_dim), dtype= 'float32')\n",
    "    cnt = 0\n",
    "\n",
    "    for word, i  in word_vocab.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            word_matrix[i] = np.asarray(embedding_vector, dtype='float32')\n",
    "        else:\n",
    "            word_matrix[i] = np.random.uniform(-0.25, 0.25, args.word_emb_dim)\n",
    "            cnt += 1\n",
    "    logger.info(f\"{cnt} words not in pretrined matrix\")\n",
    "\n",
    "    word_matrix = torch.from_numpy(word_matrix)\n",
    "    return word_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,\n",
    "                                 max_seq_len,\n",
    "                                 max_word_len,\n",
    "                                 word_vocab,\n",
    "                                 char_vocab,\n",
    "                                 label_vocab,\n",
    "                                 pad_token = 'PAD',\n",
    "                                 unk_token = 'UNK'):\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 5000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        \n",
    "        # Convert tokens to idx & Padding\n",
    "        word_pad_idx, char_pad_idx, label_pad_idx = word_vocab[pad_token], char_vocab[pad_token], label_vocab[pad_token]\n",
    "        word_unk_idx, char_unk_idx, label_unk_idx = word_vocab[unk_token], char_vocab[unk_token], label_vocab[unk_token]\n",
    "\n",
    "        word_ids = []\n",
    "        char_ids = []\n",
    "        label_ids = []\n",
    "\n",
    "        for word in example.words:\n",
    "            word_ids.append(word_vocab.get(word, word_unk_idx))\n",
    "            ch_in_word = []\n",
    "            for char in word:\n",
    "                ch_in_word.append(char_vocab.get(char, char_unk_idx))\n",
    "            #Padding for char\n",
    "            char_padding_length = max_word_len - len(ch_in_word)\n",
    "            ch_in_word = ch_in_word + ([char_pad_idx] * char_padding_length)\n",
    "            ch_in_word = ch_in_word[:max_word_len]\n",
    "            char_ids.append(ch_in_word)\n",
    "\n",
    "        for label in example.labels:\n",
    "            label_ids.append(label_vocab.get(label, label_unk_idx))\n",
    "\n",
    "        mask = [1] * len(word_ids)\n",
    "\n",
    "        #Padding for word and label\n",
    "        word_padding_lenght = max_seq_len - len(word_ids)\n",
    "        word_ids = word_ids + ([word_pad_idx] * word_padding_lenght)\n",
    "        label_ids - label_ids + ([label_pad_idx] * word_padding_lenght)\n",
    "        mask = mask + ([0] * word_padding_lenght)\n",
    "\n",
    "        word_ids = word_ids[:max_seq_len]\n",
    "        char_ids = char_ids[:max_seq_len]\n",
    "        label_ids = label_ids[:max_seq_len]\n",
    "        mask = mask[:max_seq_len]\n",
    "\n",
    "        # Additional padding for char if word_padding_length > 0\n",
    "        if word_padding_lenght > 0:\n",
    "            for _ in range(word_padding_lenght):\n",
    "                char_ids.append([char_pad_idx] * max_word_len)\n",
    "\n",
    "        \n",
    "        # Verify 가정 설정문 /  값을 보증하는 방식으로 코딩\n",
    "        assert len(word_ids) == max_seq_len, f\"Error with word_ids length {len(word_ids)} vs {max_seq_len}\"\n",
    "        assert len(char_ids) == max_seq_len, f\"Error with char_ids length {len(char_ids)} vs {max_seq_len}\"\n",
    "        assert len(label_ids) == max_seq_len, f\"Error with label_ids length {len(label_ids)} vs {max_seq_len}\"\n",
    "        assert len(mask) == max_seq_len, f\"Error with mask length {len(mask)} vs {max_seq_len}\"\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid : %s\" % example.guid)\n",
    "            logger.info(\"words: %s\" % \" \".join([str(x) for x in example.words]))\n",
    "            logger.info(\"word_ids: %s\" % \" \".join([str(x) for x in word_ids]))\n",
    "            logger.info(\"char_ids[0]: %s\" % \" \".join([str(x) for x in char_ids[0]]))\n",
    "            logger.info(\"mask: %s\" % \" \".join([str(x) for x in mask]))\n",
    "            logger.info(\"label_ids: %s\" % \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputExample(word_ids = word_ids,\n",
    "                        char_ids = char_ids,\n",
    "                        mask = mask,\n",
    "                        label_ids = label_ids)\n",
    "\n",
    "        )\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_examples(args, mode):\n",
    "    processor = NaverNerProcessor(args)\n",
    "\n",
    "    # Load data features from dataset file\n",
    "    logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "    if mode == \"train\":\n",
    "        examples = processor.get_examples(\"train\")\n",
    "    elif mode == \"dev\":\n",
    "        examples = processor.get_examples(\"dev\")\n",
    "    elif mode == \"test\":\n",
    "        examples = processor.get_examples(\"test\")\n",
    "    else:\n",
    "        raise Exception(\"For mode, Only train, dev, test is available\")\n",
    "\n",
    "    word_vocab, char_vocab, _, _ = load_vocab(args)\n",
    "    label_vocab = load_label_vocab(args)\n",
    "\n",
    "    features = convert_examples_to_features(examples,\n",
    "                                            args.max_seq_len,\n",
    "                                            args.max_word_len,\n",
    "                                            word_vocab,\n",
    "                                            char_vocab,\n",
    "                                            label_vocab)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_word_ids = torch.tensor([f.word_ids for f in features], dtype=torch.long)\n",
    "    all_char_ids = torch.tensor([f.char_ids for f in features], dtype=torch.long)\n",
    "    all_mask = torch.tensor([f.mask for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    logger.info(f\"all_word_ids.size(): {all_word_ids.size()}\")\n",
    "    logger.info(f\"all_char_ids.size(): {all_char_ids.size()}\")\n",
    "    logger.info(f\"all_mask.size(): {all_mask.size()}\")\n",
    "    logger.info(f\"all_label_ids.size(): {all_label_ids.size()}\")\n",
    "\n",
    "    dataset = TensorDataset(all_word_ids, all_char_ids, all_mask, all_label_ids)\n",
    "    return dataset\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a44d44f71612b50f5c792a4f57cb373375893a6c1b3e54652410687066e7861"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
