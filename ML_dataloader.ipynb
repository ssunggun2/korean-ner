{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomImageDataset(Dataset):\n",
    "\n",
    "\n",
    "    \n",
    "#     def __init__(self, , , transform=None, target_transform=None):\n",
    "#         self.img_labels = pd.read_csv(, names=['file_name', 'label'])\n",
    "#         self.img_dir = \n",
    "#         self.transform = transform\n",
    "#         self.target_transform = target_transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.img_labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "#         image = (img_path)\n",
    "#         label = self.img_labels.iloc[idx, 1]\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         if self.target_transform:\n",
    "#             label = self.target_transform(label)\n",
    "#         return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, words, labels):\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.lables = labels\n",
    "    \n",
    "    def __repr(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serialize this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serialize this instance to a Json string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + '\\n'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-8-88955feb57a7>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-88955feb57a7>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    def to_dict(self):\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, word_ids, char_ids, mask, label_ids):\n",
    "        self.word_ids = word_ids\n",
    "        self.char_ids = char_ids\n",
    "        self.mask = mask\n",
    "        self.label_ids = label_ids\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python diction\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dump(self.to_dict(), intent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaverNerProcessor(object):\n",
    "    \"\"\"Processor for thr Naver NER data set\"\"\"\n",
    "\n",
    "    def __init__(self, args)):\n",
    "        self.args = args\n",
    "        self.labels_lst = get_labels(args)\n",
    "\n",
    "    @classmethod\n",
    "    def _read_file(cls, input_file):\n",
    "        \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            lines = []\n",
    "            for line in f:\n",
    "                lines.append(line.strip())\n",
    "            return lines\n",
    "    \n",
    "    def _create_examples(self, dataset, set_type):\n",
    "        \"\"\"Create examples for thr training and dev sets\"\"\"\n",
    "        examples = []\n",
    "        for (i, data) in enumerate(dataset):\n",
    "            words, labels = data.split('\\t')\n",
    "            words = words.split()\n",
    "            labels = labels.split()\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "\n",
    "            assert len(words) == len(labels)\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                logger.info(data)\n",
    "            examples.append(InputExample(guid = guid, words = words, labels = labels))\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: trina, dev, test\n",
    "        \"\"\"\n",
    "\n",
    "        file_to_read = None\n",
    "        if mode == \"train\":\n",
    "            file_to_read = self.args.train_file\n",
    "        elif mode == \"dev\":\n",
    "            file_to_read = self.args.dev_file\n",
    "        elif mode == \"test\":\n",
    "            file_to_read = self.args.test_file\n",
    "\n",
    "        logger.info(f\"LOOKING AT {(os.path.join(self.args.data_dif, file_to_read))}\")\n",
    "        return self._create_examples(self._read_file(os.path.join(self.args.data_dir, file_to_read)), mode)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_matrix(args, word_vocab):\n",
    "    if not os.path.exists(args.wordvec_dir):\n",
    "        os.mkdir(args.wordvec_dir)\n",
    "\n",
    "    # Making new word vector (as list type)\n",
    "    logger.info(\"Building word matrix...\")\n",
    "    embedding_index = dict()\n",
    "    with open(os.path.join(args.wordvec_dir, args.w2v_file) , 'r', encoding='utf-8', errors = 'ignore') as f:\n",
    "        for line in f:\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            embedding_index[tokens[0]] = list(map(float, tokens[1:]))\n",
    "\n",
    "    word_matrix = np.zeros((args.word_vocab_size, args.word_emb_dim), dtype= 'float32')\n",
    "    cnt = 0\n",
    "\n",
    "    for word, i  in word_vocab.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            word_matrix[i] = np.asarray(embedding_vector, dtype='float32')\n",
    "        else:\n",
    "            word_matrix[i] = np.random.uniform(-0.25, 0.25, args.word_emb_dim)\n",
    "            cnt += 1\n",
    "    logger.info(f\"{cnt} words not in pretrined matrix\")\n",
    "\n",
    "    word_matrix = torch.from_numpy(word_matrix)\n",
    "    return word_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a44d44f71612b50f5c792a4f57cb373375893a6c1b3e54652410687066e7861"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
