{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, DataLoader\n",
    "\n",
    "from ML_model import BiLSTM_CNN_CRF\n",
    "from ML_dataloader import load_word_matrix\n",
    "from ML_utils import get_labels, get_test_texts, set_seed, compute_metrics, show_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.label_lst = get_labels(args)\n",
    "        self.num_labels = len(self.label_lst)\n",
    "\n",
    "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "        self.pad_token_label_id = args.ignore_index\n",
    "\n",
    "        self.pretrained_word_matrix = None\n",
    "        if not args.no_w2v:\n",
    "            self.pretrained_word_matrix = load_word_matrix(args, self.word_vocab)\n",
    "\n",
    "        self.model = BiLSTM_CNN_CRF(args, self.pretrained_word_matrix)\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.test_texts = None\n",
    "        if args.write_pred:\n",
    "            self.test_texts = get_test_texts(args)\n",
    "            # Empty the origianl prediction files\n",
    "            if os.path.exists(args.pred_dir):\n",
    "                shutil.rmtree(args.pred_dir)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size = self.args.train_batch_size)\n",
    "\n",
    "        # Optimizer and schedule (linear warmup and decay)\n",
    "        optimizer = Adam(self.model.parameters(), lr = self.args.learning_rate)\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running Training *****\")\n",
    "        logger.info(f\"   Num examples = {len(self.train_dataset)}\")\n",
    "        logger.info(f\"   Num Epochs = {self.args.num_train_epochs}\")\n",
    "        logger.info(f\"   Batch size = {self.args.train_batch_size}\")\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args.num_train_epochs), desc = \"Epoch\")\n",
    "        set_seed(self.args)\n",
    "\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc = \"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)         # GPU or CPU\n",
    "\n",
    "                inputs = {'word_ids' : batch[0],\n",
    "                          'char_ids' : batch[1],\n",
    "                          'mask' : batch[2],\n",
    "                          'label_ids' : batch[3]}\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs[0]\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "\n",
    "                optimizer.step()\n",
    "                self.model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
    "                    self.evaluate(\"test\", global_step)\n",
    "\n",
    "                if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
    "                    self.save_model()\n",
    "\n",
    "        return global_step, tr_loss / global_step\n",
    "\n",
    "    def evaluate(self, mode, step):\n",
    "        if mode == 'test':\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == 'dev':\n",
    "            dataset = self.dev_dataset\n",
    "        elif mode == 'train':\n",
    "            dataset = self.train_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only train, dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler =  eval_sampler, batch_size = self.args.eval_batch_size)\n",
    "        \n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        logger.info(\"   Num examples = %d\", len(dataset))\n",
    "        logger.inof(\"   Batch size = %d\", self.args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_stpes = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc = \"Evaluating\"):\n",
    "            self.model.eval()\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    'word_ids' : batch[0],\n",
    "                    'char_ids' : batch[1],\n",
    "                    'mask' : batch[2],\n",
    "                    'label_ids' : batch[3]\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "\n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_stpes += 1\n",
    "\n",
    "            # Slot prediction\n",
    "            if preds is None:\n",
    "                # decode() in 'torchcrf' returns list with best index directly\n",
    "                preds = np.array(self.model.crf.decode(logits, mask=inputs['mask'].byte()))\n",
    "                out_label_ids  = inputs[\"label_ids\"].detach().cpu().numpy()\n",
    "            else:\n",
    "                preds = np.append(preds, np.array(self.model.crf.decode(logits, mask=inputs['mask'].byte())), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs[\"label_ids\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_stpes\n",
    "        results = {\n",
    "            \"loss\" : eval_loss\n",
    "        }\n",
    "\n",
    "        # Slot result\n",
    "        slot_label_map = {i: label for i, label in enumerate(self.label_lst)}\n",
    "        out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "        preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "        for i in range(out_label_ids.shape[0]):\n",
    "            for j in range(out_label_ids.shape[1]):\n",
    "                if out_label_ids[i, j] != self.pad_token_label_id:\n",
    "                    out_label_list[i].append(slot_label_map[out_label_ids[i][j]])\n",
    "                    preds_list[i].append(slot_label_map[preds[i][j]])\n",
    "\n",
    "        if self.args.write_pred:\n",
    "            if not os.path.exists(self.args.pred_dir):\n",
    "                os.mkdir(self.args.pred_dir)\n",
    "\n",
    "            with open(os.path.join(self.args.pred_dir, \"pred_{}.txt\".format(step)), \"w\", encoding=\"utf-8\") as f:\n",
    "                for text, true_label, pred_label in zip(self.test_texts, out_label_list, preds_list):\n",
    "                    for t, tl, pl in zip(text, true_label, pred_label):\n",
    "                        f.write(\"{} {} {}\\n\".format(t, tl, pl))\n",
    "                    f.write(\"\\n\")\n",
    "\n",
    "        result = compute_metrics(out_label_list, preds_list)\n",
    "        results.update(result)\n",
    "\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(results.keys()):\n",
    "            logger.info(\"   %s = %s\", key, str(results[key]))\n",
    "        logger.info(\"\\n\" + show_report(out_label_list, preds_list))     # Get the report for each tag result\n",
    "\n",
    "        return results\n",
    "    \n",
    "    def save_model(self):\n",
    "        # Save model checkpoint (Overwrite)\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            os.mkdir(self.args.model_dir)\n",
    "\n",
    "        \n",
    "        # Save argument\n",
    "        torch.save(self.args, os.path.join(self.args.model_dir, 'args.pt'))\n",
    "        # Save model for inference\n",
    "        torch.save(self.model.state_dict(), os.path.join(self.args.model_dir, 'model.pt'))\n",
    "        logger.info(\"Saving model checkpoint to {}\".format(os.path.join(self.args.model_dir, 'model.pt')))\n",
    "\n",
    "    def load_model(self):\n",
    "        # Check whether model exists\n",
    "        if not os.path.exists(self.args.model_dir):\n",
    "            raise Exception(\"Model dosen't exists! Train first!\")\n",
    "\n",
    "        try:\n",
    "            # self.bert_config = self.config_class.from_pretrained(self.args.model_dir)\n",
    "            self.args = torch.load(os.path.join(self.args.model_dir, 'args.pt'))\n",
    "            logger.info(\"***** Args loaded *****\")\n",
    "            self.model.load_state_dict(torch.load(os.path.join(self.args.model_dir, 'model.pt')))\n",
    "            self.model.to(self.device)\n",
    "            logger.info(\"***** Model Loaded *****\")\n",
    "        except:\n",
    "            raise Exception(\"Some model files might be missing...\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a44d44f71612b50f5c792a4f57cb373375893a6c1b3e54652410687066e7861"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
