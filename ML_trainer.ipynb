{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\USER\\anaconda3\\envs\\env\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import logging\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import RandomSampler, SequentialSampler, DataLoader\n",
    "\n",
    "from ML_model import BiLSTM_CNN_CRF\n",
    "from ML_dataloader import load_word_matrix\n",
    "from ML_utils import get_labels, get_test_texts, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
    "        self.args = args\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "        self.label_lst = get_labels(args)\n",
    "        self.num_labels = len(self.label_lst)\n",
    "\n",
    "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
    "        self.pad_token_label_id = args.ignore_index\n",
    "\n",
    "        self.pretrained_word_matrix = None\n",
    "        if not args.no_w2v:\n",
    "            self.pretrained_word_matrix = load_word_matrix(args, self.word_vocab)\n",
    "\n",
    "        self.model = BiLSTM_CNN_CRF(args, self.pretrained_word_matrix)\n",
    "\n",
    "        # GPU or CPU\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.test_texts = None\n",
    "        if args.write_pred:\n",
    "            self.test_texts = get_test_texts(args)\n",
    "            # Empty the origianl prediction files\n",
    "            if os.path.exists(args.pred_dir):\n",
    "                shutil.rmtree(args.pred_dir)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        train_sampler = RandomSampler(self.train_dataset)\n",
    "        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size = self.args.train_batch_size)\n",
    "\n",
    "        # Optimizer and schedule (linear warmup and decay)\n",
    "        optimizer = Adam(self.model.parameters(), lr = self.args.learning_rate)\n",
    "\n",
    "        # Train!\n",
    "        logger.info(\"***** Running Training *****\")\n",
    "        logger.info(f\"   Num examples = {len(self.train_dataset)}\")\n",
    "        logger.info(f\"   Num Epochs = {self.args.num_train_epochs}\")\n",
    "        logger.info(f\"   Batch size = {self.args.train_batch_size}\")\n",
    "\n",
    "        global_step = 0\n",
    "        tr_loss = 0.0\n",
    "        self.model.zero_grad()\n",
    "\n",
    "        train_iterator = trange(int(self.args.num_train_epochs), desc = \"Epoch\")\n",
    "        set_seed(self.args)\n",
    "\n",
    "        for _ in train_iterator:\n",
    "            epoch_iterator = tqdm(train_dataloader, desc = \"Iteration\")\n",
    "            for step, batch in enumerate(epoch_iterator):\n",
    "                self.model.train()\n",
    "                batch = tuple(t.to(self.device) for t in batch)         # GPU or CPU\n",
    "\n",
    "                inputs = {'word_ids' : batch[0],\n",
    "                          'char_ids' : batch[1],\n",
    "                          'mask' : batch[2],\n",
    "                          'label_ids' : batch[3]}\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs[0]\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                tr_loss += loss.item()\n",
    "\n",
    "                optimizer.step()\n",
    "                self.model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
    "                    self.evaluate(\"test\", global_step)\n",
    "\n",
    "                if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
    "                    self.save_model()\n",
    "\n",
    "        return global_step, tr_loss / global_step\n",
    "\n",
    "    def evaluate(self, mode, step):\n",
    "        if mode == 'test':\n",
    "            dataset = self.test_dataset\n",
    "        elif mode == 'dev':\n",
    "            dataset = self.dev_dataset\n",
    "        elif mode == 'train':\n",
    "            dataset = self.train_dataset\n",
    "        else:\n",
    "            raise Exception(\"Only train, dev and test dataset available\")\n",
    "\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler =  eval_sampler, batch_size = self.args.eval_batch_size)\n",
    "        \n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
    "        logger.info(\"   Num examples = %d\", len(dataset))\n",
    "        logger.inof(\"   Batch size = %d\", self.args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_stpes = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "\n",
    "        for batch in tqdm(eval_dataloader, desc = \"Evaluating\"):\n",
    "            self.model.eval()\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {\n",
    "                    'word_ids' : batch[0],\n",
    "                    'char_ids' : batch[1],\n",
    "                    'mask' : batch[2],\n",
    "                    'label_ids' : batch[3]\n",
    "                }\n",
    "                outputs = self.model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a44d44f71612b50f5c792a4f57cb373375893a6c1b3e54652410687066e7861"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
